{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recitation_10_– Linear_Regression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M_sj9n9iJwxh","colab_type":"text"},"source":["# Welcome to Recitation 10!\n"]},{"cell_type":"markdown","metadata":{"id":"OlzxLO6PifrR","colab_type":"text"},"source":["#**Go Read `Rec10-Intro.pdf` if you have not already.** \n","\n","This lab will not make sense without it! "]},{"cell_type":"markdown","metadata":{"id":"C5fjXrHwJlrQ","colab_type":"text"},"source":["## RECITATION ASSIGNMENT\n","\n","This lab project will guide you through the analysis."]},{"cell_type":"markdown","metadata":{"id":"yb2ZzEpLJey_","colab_type":"text"},"source":["## Data\n","\n","To obtain data on how various variable values affect pouring and cooling, a batch\n","of 100 castings is poured with random variations in the mold variables about their baseline\n","values. The data are available in the file `castdata.csv` on github. Each row contains parameter\n","values (the inputs), and the cast batch time. The first line in the file contains the header\n","with the names of the variables. The data start in the second row. The first row of data has\n","the baseline values, that is, the values of the variables used in the current casting approach."]},{"cell_type":"markdown","metadata":{"id":"vUX8BCsRHoxe","colab_type":"text"},"source":["## Variables\n","\n","The following variables can be varied: `Riser Height`, `Riser Diameter`, `Riser 1 Position`, `Riser 2 Position`, `Gate Diameter`, `Cup Height`, `Sprue Height`, `Sprue Diameter Bottom`, and `Sprue Diameter Top` (see Figure 2). The response variable is “`BatchTime`”."]},{"cell_type":"markdown","metadata":{"id":"YtTGj_lXJu7C","colab_type":"text"},"source":["## Importing Data\n","\n","We first need to import the libraries necessary for this recitation. Next, we have to upload our data into the notebook; Upload the data set `castdata.csv` from the course github. We will call the entire data set `df`."]},{"cell_type":"code","metadata":{"id":"CoP3YPMlJj1y","colab_type":"code","colab":{}},"source":["#math and arrays\n","import numpy as np\n","#dataframes\n","import pandas as pd\n","#plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","#linear regression and model selection \n","import statsmodels.api as sm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rg3kqVdqM9Sf","colab_type":"code","colab":{}},"source":["# 'df' is common pandas dataframe nomenclature\n","df = pd.read_csv('castdata.csv')\n","df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MAcZH9TxN0z9","colab_type":"text"},"source":["## Visual Analysis\n","\n","To examine the relationships between all pairs of variables (\"`feasible`\" is ignored through the use of `df.loc`, which splices the dataframe in the same way that one would a regular Python list), plot the dataframe with the pairplot command using the seaborn library:"]},{"cell_type":"code","metadata":{"id":"IwRd1XUsOz4i","colab_type":"code","colab":{}},"source":["sns.pairplot(df.loc[:, :'BatchTime'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULQeEoi7T-A0","colab_type":"text"},"source":["When you pairplot a dataframe you get a scatterplot matrix. This matrix allows you to visualize all pairwise relationships between variables."]},{"cell_type":"markdown","metadata":{"id":"yBCm9YTPS_Cf","colab_type":"text"},"source":["## Question 1\n","\n","Based on the scatterplot matrix, which variable of the first 9 in df most affects \"`BatchTime`\"?"]},{"cell_type":"markdown","metadata":{"id":"-ORuNQrdreNs","colab_type":"text"},"source":["Ans:"]},{"cell_type":"markdown","metadata":{"id":"xmKqZBa3TO8q","colab_type":"text"},"source":["## Multiple Linear Regression\n","\n","We will now fit a multiple linear regression model for batch time using the mold variables as predictors. You'll see in the following code that we first import the statsmodels library. We then assign 'batchime' to be our dependent variable and 'X' to be our independent variables by splicing our dataframe as we did before. The line `X = sm.add_constant(X)` adds a column of all ones called `const` to X. This is needed to fit the intercept. \n","\n","Next, we create a regression model and get the results of the regression using model.fit(). Finally, we summarize these results. Run the code below to view the summary table."]},{"cell_type":"code","metadata":{"id":"CLThCXQQaP6x","colab_type":"code","colab":{}},"source":["batchtime = df[\"BatchTime\"]\n","X = df.loc[:, :'Riser2Pos']\n","X = sm.add_constant(X) \n","\n","model = sm.OLS(batchtime, X)\n","results = model.fit()\n","\n","# Print out the stats\n","results.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cNgBcc1-bKpA","colab_type":"text"},"source":["This summary output may seem intimidating at first; let's focus on the independent variables (section that vertically lists our variables with \"coef\", \"std err\", ...). Specifically, we are interested in the column titled $P>|t|$, which gives the p-value or statistical significance for that variable. Here's a way to interpret these values: a p-value of .01 would indicate statistical significance at the 99% level for that variable. In determining the degree of effect a variable has on `BatchTime`, simply observe how large the coefficent is; this means that a small change in that variable has a significant impact on our dependent variable, `BatchTime`."]},{"cell_type":"markdown","metadata":{"id":"p7YM_G7Be0ML","colab_type":"text"},"source":["## Question 2\n","Observe the summary output in the model for `BatchTime`. Which variables appear to be statistically significant at the 95% level, and how do they affect the `BatchTime`? Which predictor(s) have the largest effect on `BatchTime`?"]},{"cell_type":"markdown","metadata":{"id":"5aJkRmZvv3Fh","colab_type":"text"},"source":["Ans:"]},{"cell_type":"markdown","metadata":{"id":"nzzQZbBqc5AY","colab_type":"text"},"source":["Now, for each of our variables, we would like to plot the fitted values from our regression versus those for the actual BatchTime using Matplotlib; run the code below and observe the graphs."]},{"cell_type":"code","metadata":{"id":"f74So4QlFCgj","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(10,figsize =(10,45))\n","resid = results.resid\n","for i in range(10): # for each of the nine variables\n","  # create a fitted plot for that variable ('i' indexes the variables)\n","  ax[i].scatter(X.iloc[:,i],resid) \n","  ax[i].set_xlabel(X.columns[i]) \n","  ax[i].set_ylabel(\"residual\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdyPbtz9eRm_","colab_type":"text"},"source":["## Question 3\n","\n","Examine the nine plots. Do any of them show nonlinear patterns? That is, does the difference between the fitted and actual values change based on the x value? Or does there appear to be a relatively uniform disparity across the plot? Describe the nonlinear patterns, if any, that you see.\n"]},{"cell_type":"markdown","metadata":{"id":"mLm1B7-Ix8Fc","colab_type":"text"},"source":["Ans: "]},{"cell_type":"markdown","metadata":{"id":"sqgp5VT7x-PG","colab_type":"text"},"source":["## Question 4\n","\n","\n","Are the other assumptions about the errors made by linear regression satisfied? In particular, address the following questions: Do the residuals appear mutually independent? Are there any problems with non-constant variance (heteroscedasticity)? Do the residuals appear normallly distributed?\n","\n","Hint 1: You may want to refer to the `test-assumptions.ipynb` demo on github for ideas of visualizations that are helpful to answer these questions. \n","\n","Hint 2: Normally distributed residuals will form a line of a qq-plot, though not necessarily the 45-degree line. The slope is dependent on the variance of the residual. Try running the code below with various choices of `sigma` to convince yourself of this.  "]},{"cell_type":"code","metadata":{"id":"2wI8-O0939t1","colab_type":"code","colab":{}},"source":["#Code for Hint 2\n","\n","sigma = 1 #rerun this code with various choices of sigma, e.g. 0.5, 5\n","\n","n=500\n","eps = sigma * np.random.randn(n) # normal residuals \n","x = 10*np.random.rand(n)\n","y = x + eps\n","model = sm.OLS(y,x).fit()\n","\n","# scipy.stats.probplot(data, dist=\"norm\", plot=plt);\n","plt.hist(eps)\n","sm.qqplot(model.resid, line='45');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YFdT5ognB4V","colab_type":"text"},"source":["Ans:"]},{"cell_type":"markdown","metadata":{"id":"cjui1LfZnDgx","colab_type":"text"},"source":["##Adding nonlinear terms\n","\n","You may have noticed that `BatchTime` is nonlinearly related to `GameDiam`. Use the following commands to repeate the regression analysis with quadratic and cubic terms. "]},{"cell_type":"code","metadata":{"id":"M2kttzRlna5U","colab_type":"code","colab":{}},"source":["X['GateDiamSquared'] = X['GateDiam']**2 #add quadratic term for GateDiam\n","X['GateDiamCubic'] = X['GateDiam']**3  #add cubic term\n","model2 = sm.OLS(batchtime, X)\n","results2 = model2.fit()\n","\n","# Print out the stats\n","results2.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"so13B4nyvWcu","colab_type":"text"},"source":["##Question 5\n","\n","Now based on the new model we have just fit, are the assumptions about the errors made satisfied? In particular, address the following questions: Do the residuals reveal any problems with the model? Are there any problems with non-constant variance (heteroscedasticity)? Do the residuals appear normall distributed?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Hr7z0G7-5HX1","colab_type":"text"},"source":["Ans:"]},{"cell_type":"markdown","metadata":{"id":"h6lMmv9t9E_j","colab_type":"text"},"source":["##Model Selection\n","\n","Now we want see if there is a good model that is simplier, i.e. it uses fewer features. \n","\n","First we must split our data into training and testing sets so that we can test our models accurately. \n","\n","Run the following code to split your data. \n"]},{"cell_type":"code","metadata":{"id":"XYFRT_oSr3Nn","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X, batchtime, test_size=0.25, random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-80Pb9UsWo_","colab_type":"text"},"source":["##Model Selection Using AIC\n","\n","Run the following code to select your model based on the training set using AIC, then test it using the testing set."]},{"cell_type":"code","metadata":{"id":"wD_bUWAdsp0b","colab_type":"code","colab":{}},"source":["def minAIC(X,y):\n","    variables = X.columns\n","    model = sm.OLS(y,X[variables]).fit()\n","    while True:\n","        maxp = np.max(model.pvalues)\n","        new_variables = variables[model.pvalues < maxp]\n","        newmodel = sm.OLS(y,X[new_variables]).fit()\n","        if newmodel.aic < model.aic:\n","            model = newmodel\n","            variables = new_variables\n","        else:\n","            break\n","    return model,variables\n","\n","# select on training set \n","model,variables = minAIC(X_train, y_train)\n","print(variables)\n","\n","\n","y_pred = model.predict(X_test[variables])\n","print(mean_squared_error(y_test,y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQxf1w3T5Jc9","colab_type":"text"},"source":["##Question 6 \n","\n","Which features does AIC select? What is the mean squared error for the testing set?"]},{"cell_type":"markdown","metadata":{"id":"Ae9eytEieA1f","colab_type":"text"},"source":["Ans:"]},{"cell_type":"markdown","metadata":{"id":"ZRxr1pCbeT5r","colab_type":"text"},"source":["##Question 7"]},{"cell_type":"markdown","metadata":{"id":"U6u5L_BYeCGM","colab_type":"text"},"source":["Now train a model on the training set that uses all the features. "]},{"cell_type":"code","metadata":{"id":"k8RuXuy7ePsj","colab_type":"code","colab":{}},"source":["#Code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ecy7nAz4eRJK","colab_type":"text"},"source":["##Question 8\n","\n","What is the mean squared error for your new model? Which model do you prefer and why?"]},{"cell_type":"code","metadata":{"id":"OYSdU52hepHB","colab_type":"code","colab":{}},"source":["#Code for MSE here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ULnKLh_eno8","colab_type":"text"},"source":["Ans:"]}]}